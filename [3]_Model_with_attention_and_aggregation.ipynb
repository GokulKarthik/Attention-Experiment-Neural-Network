{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[3]_Model_with_attention_and_aggregation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9Tfb-_l7ivwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "metadata": {
        "id": "zuGbVGVQiJqe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c0e987d-a1eb-4649-d6a1-90e86c3d8c3b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531908202919,
          "user_tz": -330,
          "elapsed": 18915,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rBFOKilPjFAe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aO-MjsH61AA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epochs_ = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_b4-HiIkivN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "mOLK_f66kp0f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_data(k=2, d=1, k_useful=1, n_rows=1500, print_param=False):\n",
        "  \n",
        "  \"\"\"\n",
        "  generate data that is suitable for attention models with no dependency between parts of the image \n",
        "  \n",
        "  :parameters\n",
        "  k - number of parts in each data point\n",
        "  d - number of sub parts in each 'k' part\n",
        "  k_useful - number of parts with useful information [k_useful <= k]\n",
        "  n_rows - number of rows in the data set\n",
        "  print_param - boolean value to set the printinf status of parametrs\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  \n",
        "  loc_useful = 0\n",
        "  scale_useful = 1\n",
        "  loc_non_useful = 2\n",
        "  scale_non_useful = 1\n",
        "  \n",
        "  is_useful_all = []\n",
        "  \n",
        "  for _ in range(n_rows):\n",
        "\n",
        "    # each element of 'is_useful' shows whether the corresponding part is useful or not\n",
        "    is_useful = np.array([0]*k)\n",
        "    useful_idx = np.random.choice(range(k), size=k_useful, replace=False)\n",
        "    for i in useful_idx:\n",
        "      is_useful[i] = 1\n",
        "    is_useful_all.append(list(is_useful))\n",
        "\n",
        "    data_point = []\n",
        "    for i in range(k):\n",
        "      data_part = []\n",
        "      if is_useful[i] == 1:\n",
        "        data_part = np.random.normal(loc=loc_useful, scale=scale_useful, size=d)\n",
        "      else:\n",
        "        #loc_non_useful = np.random.rand()\n",
        "        #scale_non_useful = np.random.choice(np.linspace(0, 100, 10))\n",
        "        data_part = np.random.normal(loc=loc_non_useful, scale=scale_non_useful, size=d)\n",
        "      data_point.append(list(data_part))\n",
        "    \n",
        "    data.append(data_point)\n",
        "    \n",
        "  if print_param:\n",
        "    print(\"number of parts: \", k)\n",
        "    print(\"useful parts: \", is_useful_all[:5])\n",
        "    print(\"loc_non_useful: \", loc_useful)\n",
        "    print(\"scale_non_useful: \", scale_useful)\n",
        "    print(\"loc_non_useful: \", loc_non_useful)\n",
        "    print(\"scale_non_useful: \", scale_non_useful)\n",
        "    print(\"-\"*50)\n",
        "                             \n",
        "  data = np.array(data)\n",
        "  data = data.round(decimals=2)\n",
        "  return data, is_useful_all, loc_useful, np.array([scale_useful]), loc_non_useful, np.array([scale_non_useful])                           \n",
        "                                  \n",
        "#data, is_useful_all = generate_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1vnS_jMl1eu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_data_2(k=2, d=1, k_useful=1, n_rows=1500, print_param=False):\n",
        "  \n",
        "  \"\"\"\n",
        "  generate data suitable for attention models with the dependency between parts of the image\n",
        "  \n",
        "  :parameters\n",
        "  k - number of parts in each data point\n",
        "  d - number of sub parts in each 'k' part\n",
        "  k_useful - number of parts with useful information [k_useful <= k]\n",
        "  n_rows - number of rows in the data set\n",
        "  print_param - boolean value to set the printinf status of parametrs\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  \n",
        "  mean_useful = [0]*d_\n",
        "  std_dev_useful = np.random.randn(d_, d_)\n",
        "  mean_non_useful = [2]*d_\n",
        "  std_dev_non_useful = np.random.randn(d_, d_)\n",
        "  \n",
        "  is_useful_all = []\n",
        "  \n",
        "  for _ in range(n_rows):\n",
        "\n",
        "    # each element of 'is_useful' shows whether the corresponding part is useful or not\n",
        "    is_useful = np.array([0]*k)\n",
        "    useful_idx = np.random.choice(range(k), size=k_useful, replace=False)\n",
        "    for i in useful_idx:\n",
        "      is_useful[i] = 1\n",
        "    is_useful_all.append(list(is_useful))\n",
        "\n",
        "    data_point = []\n",
        "    for i in range(k):\n",
        "      data_part = []\n",
        "      if is_useful[i] == 1:\n",
        "        data_part = np.matmul(np.random.randn(d_), std_dev_useful) + mean_useful\n",
        "        #data_part = np.random.normal(loc=loc_useful, scale=scale_useful, size=d)\n",
        "      else:\n",
        "        data_part = np.matmul(np.random.randn(d_), std_dev_non_useful) + mean_non_useful\n",
        "        #loc_non_useful = np.random.rand()\n",
        "        #scale_non_useful = np.random.choice(np.linspace(0, 100, 10))\n",
        "        #data_part = np.random.normal(loc=loc_non_useful, scale=scale_non_useful, size=d)\n",
        "      data_point.append(list(data_part))\n",
        "    \n",
        "    data.append(data_point)\n",
        "    \n",
        "  if print_param:\n",
        "    print(\"number of parts: \", k)\n",
        "    print(\"useful parts: \", is_useful_all[:5])\n",
        "    print(\"mean_non_useful: \", mean_useful)\n",
        "    print(\"std_dev_non_useful: \", std_dev_useful)\n",
        "    print(\"mean_non_useful: \", mean_non_useful)\n",
        "    print(\"std_dev_non_useful: \", std_dev_non_useful)\n",
        "    print(\"-\"*50)\n",
        "                             \n",
        "  data = np.array(data)\n",
        "  data = data.round(decimals=2)\n",
        "  return data, is_useful_all, mean_useful, std_dev_useful, mean_non_useful, std_dev_non_useful   \n",
        "                                     \n",
        "                                  \n",
        "#data, is_useful_all = generate_data_2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dEQn5cVnjNc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def find_y(x, is_useful_all):\n",
        "  \n",
        "  \"\"\"\n",
        "  find 'y' (class) based on the useful part\n",
        "  \n",
        "  :parameters\n",
        "  x - data features\n",
        "  is_useful_all - boolean nxk array\n",
        "  d - number of sub parts in each 'k' part of each data point\n",
        "  \"\"\"\n",
        "  x_useful = []\n",
        "  d = x.shape[2]\n",
        "  \n",
        "  for row, is_useful in zip(x, is_useful_all):\n",
        "    useful_sum = np.array([0.]*d)\n",
        "    for idx, value in enumerate(is_useful):\n",
        "      if value == 1:\n",
        "        useful_sum += row[idx]\n",
        "    x_useful.append(useful_sum)\n",
        "    \n",
        "  x_useful = np.array(x_useful)\n",
        "  w = np.random.uniform(low=-1, high=1, size=(d, 1))\n",
        "  #print(x_useful.shape, w.shape)\n",
        "  # n_rows*d, d*1\n",
        "  # print(x_useful, w)\n",
        "  y = np.matmul(x_useful, w)\n",
        "  y = (y>0).astype(int)\n",
        "  return y\n",
        "\n",
        "#y = find_y(data, is_useful_all)\n",
        "#print(y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdCL-VjX311i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_useful_data(data, is_useful_all):\n",
        "  '''\n",
        "  Warning: Works only when n_useful=1\n",
        "  '''\n",
        "  data_useful = []\n",
        "\n",
        "  for row, is_useful in zip(data, is_useful_all):\n",
        "    for idx, value in enumerate(is_useful):\n",
        "      if value == 1:\n",
        "        data_useful.append(row[idx])\n",
        "        break\n",
        "\n",
        "  data_useful = np.array(data_useful)\n",
        "  \n",
        "  return data_useful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KmDNDMj2ZuB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def display_plot(data, is_useful_all, y_pred_int, y_int, plot_type):\n",
        "  \n",
        "  '''\n",
        "  plot_type: \n",
        "  0 (special_case - all [2d]), \n",
        "  1 (special_case - useful [1d]), \n",
        "  2 (not_special_case - useful [2d])\n",
        "  '''\n",
        "  \n",
        "  f1 = None\n",
        "  f2 = None\n",
        "  if plot_type == 0:\n",
        "    f1 = data.reshape(-1,2)[:,0]\n",
        "    f2 = data.reshape(-1,2)[:,0]\n",
        "  elif plot_type == 1:\n",
        "    data_useful = get_useful_data(data, is_useful_all)\n",
        "    f1 = data_useful[:,0]\n",
        "    f2 = np.ones(len(f1))\n",
        "  else:\n",
        "    data_useful = get_useful_data(data, is_useful_all)\n",
        "    f1 = data_useful[:,0]\n",
        "    f2 = data_useful[:,1]\n",
        "  \n",
        "  plt.figure(figsize=(10,5))\n",
        "  \n",
        "  plt.subplot(121)\n",
        "  plt.scatter(f1, f2, c=y_pred_int, label=y_pred_int, s=3, cmap='RdYlGn', alpha=0.5)\n",
        "  plt.xlabel(\"f1\")\n",
        "  plt.ylabel(\"f2\")\n",
        "  plt.title(\"[Predicted] Epoch \" + str(i))\n",
        "  #plt.legend()\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.scatter(f1, f2, c=y_int, label=y_int, s=3, cmap='RdYlGn', alpha=0.5)\n",
        "  plt.xlabel(\"f1\")\n",
        "  plt.ylabel(\"f2\")\n",
        "  plt.title(\"[Real] Epoch \" + str(i))\n",
        "  #plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5He0aVJ7OL-X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def log_loss(y, y_pred):\n",
        "  return np.round(np.sum(-y*np.log(y_pred)), decimals=2)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdU43Rgqy-ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "CQS6ggYqy9RU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# both before and after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model1(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model1, self).__init__()\n",
        "    #n_nodes = 2\n",
        "    ip_n = d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "    '''\n",
        "    print(\"Weights\")\n",
        "    print(self.h1_layer.weight.size())\n",
        "    print(self.h1_layer.weight.data)\n",
        "    print(self.h2_layer.weight.data)\n",
        "    print(self.op_layer.weight.data)\n",
        "    '''\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    '''\n",
        "    print(\"Bias\")\n",
        "    print(self.h1_layer.bias.size())\n",
        "    print(self.h1_layer.bias.data)\n",
        "    print(self.h2_layer.bias.data)\n",
        "    print(self.op_layer.bias.data)\n",
        "    '''\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    #softmax_res.append(op_softmax)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    attention_capture = attention_capture.flatten()\n",
        "    #print(attention_capture.shape)\n",
        "    #print(attention_capture); \n",
        "    #sys.exit()\n",
        "    \n",
        "    attention = x*op_softmax\n",
        "    #print(\"attn: \", attention.shape); input()\n",
        "    attention_new = attention.sum(1)\n",
        "    op_h2_layer = self.relu(self.h2_layer(attention_new))\n",
        "    y_pred = self.relu(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    return attention_capture, y_pred\n",
        "    \n",
        "#model = Model1(n_nodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JxR3xEIfQUhB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# only before\n",
        "softmax_res = [] \n",
        "\n",
        "class Model2(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model2, self).__init__()\n",
        "    #n_nodes = 2\n",
        "    ip_n = d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    #h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    #self.h2_layer = torch.nn.Linear(k_*d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(d_, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    #h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    #self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, k_*d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, d_)))\n",
        "    '''\n",
        "    print(\"Weights\")\n",
        "    print(self.h1_layer.weight.size())\n",
        "    print(self.h1_layer.weight.data)\n",
        "    print(self.h2_layer.weight.data)\n",
        "    print(self.op_layer.weight.data)\n",
        "    '''\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    #self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    '''\n",
        "    print(\"Bias\")\n",
        "    print(self.h1_layer.bias.size())\n",
        "    print(self.h1_layer.bias.data)\n",
        "    print(self.h2_layer.bias.data)\n",
        "    print(self.op_layer.bias.data)\n",
        "    '''\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    #softmax_res.append(op_softmax)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    attention_capture = attention_capture.flatten()\n",
        "    \n",
        "    attention = x*op_softmax\n",
        "    #print(\"attn: \", attention.shape); input()\n",
        "    attention_new = attention.sum(1) \n",
        "    #op_h2_layer = self.relu(self.h2_layer(attention))\n",
        "    y_pred = self.relu(self.op_layer(attention_new))\n",
        "      \n",
        "    return attention_capture, y_pred\n",
        "    \n",
        "#model = Model2(n_nodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZmhEb8_QUvW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# only after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model3(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model3, self).__init__()\n",
        "    #n_nodes = 2\n",
        "    ip_n = d_\n",
        "    #h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    #self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(ip_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # deviation\n",
        "    #h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    #self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, ip_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "    '''\n",
        "    print(\"Weights\")\n",
        "    print(self.h1_layer.weight.size())\n",
        "    print(self.h1_layer.weight.data)\n",
        "    print(self.h2_layer.weight.data)\n",
        "    print(self.op_layer.weight.data)\n",
        "    '''\n",
        "\n",
        "    # bias init\n",
        "    #self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    '''\n",
        "    print(\"Bias\")\n",
        "    print(self.h1_layer.bias.size())\n",
        "    print(self.h1_layer.bias.data)\n",
        "    print(self.h2_layer.bias.data)\n",
        "    print(self.op_layer.bias.data)\n",
        "    '''\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    #op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(x))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    #softmax_res.append(op_softmax)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    attention_capture = attention_capture.flatten()\n",
        "    \n",
        "   \n",
        "    attention = x*op_softmax\n",
        "    #print(\"attn: \", attention.shape); input()\n",
        "    attention_new = attention.sum(1)\n",
        "    op_h2_layer = self.relu(self.h2_layer(attention_new))\n",
        "    y_pred = self.relu(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    return attention_capture, y_pred\n",
        "    \n",
        "#model = Model3(n_nodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DV8Hu-40QU24",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# no before and after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model4(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model4, self).__init__()\n",
        "    #n_nodes = 2\n",
        "    ip_n = d_\n",
        "    #h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    #h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    #self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(ip_n, hmid_n)\n",
        "    #self.h2_layer = torch.nn.Linear(k_*d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(d_, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # deviation\n",
        "    #h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    #h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    #self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, ip_n)))\n",
        "    #self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, k_*d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, d_)))\n",
        "    '''\n",
        "    print(\"Weights\")\n",
        "    print(self.h1_layer.weight.size())\n",
        "    print(self.h1_layer.weight.data)\n",
        "    print(self.h2_layer.weight.data)\n",
        "    print(self.op_layer.weight.data)\n",
        "    '''\n",
        "\n",
        "    # bias init\n",
        "    #self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    #self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    '''\n",
        "    print(\"Bias\")\n",
        "    print(self.h1_layer.bias.size())\n",
        "    print(self.h1_layer.bias.data)\n",
        "    print(self.h2_layer.bias.data)\n",
        "    print(self.op_layer.bias.data)\n",
        "    '''\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    #op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(x))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    #softmax_res.append(op_softmax)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    attention_capture = attention_capture.flatten()\n",
        "    \n",
        "    attention = x*op_softmax\n",
        "    #print(\"attn: \", attention.shape); input()\n",
        "    attention_new = attention.sum(1)\n",
        "    #op_h2_layer = self.relu(self.h2_layer(attention))\n",
        "    y_pred = self.relu(self.op_layer(attention_new))\n",
        "      \n",
        "    return attention_capture, y_pred\n",
        "    \n",
        "#model = Model4(n_nodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OhwS8aGHjb4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ]
    },
    {
      "metadata": {
        "id": "MGSxXVRljYf3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "990cfa3c-80f9-4dba-c1e0-deae03d4d1c3",
        "executionInfo": {
          "status": "error",
          "timestamp": 1531917341300,
          "user_tz": -330,
          "elapsed": 2113,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "result = []\n",
        "cnt = 0\n",
        "attention_capture = None\n",
        "\n",
        "k_d_list = [(2,1), (25,2), (25,100)] # list of tuples in (k, d) format\n",
        "for k_, d_ in k_d_list[:2]: ##I1\n",
        "  #result[str((k_, d_))] = {}\n",
        "  is_special_case = (k_==2 and d_==1)\n",
        "  \n",
        "  n_ = max(k_*d_*30, 1500)\n",
        " \n",
        "  data, is_useful_all, mean_useful, std_dev_useful, mean_non_useful, std_dev_non_useful    = generate_data_2(k=k_, d=d_, n_rows=n_)\n",
        "  y = find_y(data, is_useful_all)\n",
        "  \n",
        "  x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split(data, y, range(n_), test_size=0.33, random_state=100, stratify=y)\n",
        "  \n",
        "  x_train_tensor = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "  x_test_tensor = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "  y_train_tensor = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "  y_test_tensor = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "  \n",
        "  is_useful_train = np.array(is_useful_all[:y_train.shape[0]])\n",
        "  is_useful_train = is_useful_train.flatten()\n",
        "  #print(is_useful_train.shape)\n",
        "  #print(is_useful_train)\n",
        "  \n",
        "  '''\n",
        "  print(data.shape)\n",
        "  print(X_train.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_train.shape)\n",
        "  print(y_test.shape)\n",
        "  input()\n",
        "  '''\n",
        "  \n",
        "  n_nodes_list = [2, 4, 8, 16] # number of nodes in every layer\n",
        "  for n_nodes in n_nodes_list[:1]: ##I2\n",
        "    #result[str((k_, d_))][str(n_nodes)] = {}\n",
        "    \n",
        "    lr_list = [0.1, 0.01, 0.001, 0.0001]\n",
        "    for lr in lr_list[:1]: ## I3\n",
        "      #result[str((k_, d_))][str(n_nodes)][str(lr)] = {}\n",
        "      \n",
        "      before_after_list = [[True, True], [True, False], [False, True], [False, False]]\n",
        "      for before, after in before_after_list[:1]: #I4\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))] = {}\n",
        "        \n",
        "        if before and after:\n",
        "          model = Model1(n_nodes=n_nodes)\n",
        "        elif before:\n",
        "          model = Model2(n_nodes=n_nodes)\n",
        "        elif after:\n",
        "          model = Model3(n_nodes=n_nodes)\n",
        "        else:\n",
        "          model = Model4(n_nodes=n_nodes)\n",
        "          \n",
        "        loss_method = torch.nn.MSELoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "      \n",
        "        # go forward and backward over the network and update parameters\n",
        "\n",
        "        display_steps = np.linspace(0,epochs_, num=5, dtype=int)\n",
        "        display_steps = np.append(display_steps, [epochs_-1])\n",
        "\n",
        "        loss_array = np.array([])\n",
        "        acc_array = np.array([])\n",
        "        attention_loss_array = np.array([])\n",
        "\n",
        "        for i in range(epochs_):\n",
        "            \n",
        "            attention_capture, y_train_pred = model.forward(x_train_tensor)\n",
        "            \n",
        "            attention_loss = log_loss(is_useful_train, attention_capture)\n",
        "            #print(attention_loss); sys.exit()\n",
        "            attention_loss_array = np.append(attention_loss_array, [attention_loss])\n",
        "\n",
        "            loss = loss_method(y_train_pred, y_train_tensor)\n",
        "            loss_array = np.append(loss_array, [loss.item()])\n",
        "\n",
        "            y_train_pred_int = (y_train_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "            y_train_int = y_train_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "            accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_train_pred_int, y_train_int)])/len(y_train_tensor)\n",
        "            acc_array = np.append(acc_array, [accuracy])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            '''\n",
        "            is_useful_all = np.array(is_useful_all)\n",
        "            if i in display_steps:\n",
        "              # visualize training\n",
        "              if not is_special_case:\n",
        "                display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=2)\n",
        "              else:\n",
        "                #display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=0)\n",
        "                display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=1)\n",
        "            '''\n",
        "         \n",
        "        attention_capture_test, y_test_pred = model.forward(x_test_tensor)\n",
        "        y_test_pred_int = (y_test_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "        y_test_int = y_test_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "        test_accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_test_pred_int, y_test_int)])/len(y_test_tensor)\n",
        "        \n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['loss_array'] = loss_array.tolist()\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['acc_array'] = acc_array.tolist()\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['test_accuracy'] = test_accuracy\n",
        "        \n",
        "        class_count = {}\n",
        "        class_count['0'] = y.flatten().tolist().count(0)\n",
        "        class_count['1'] = y.flatten().tolist().count(1)\n",
        "        \n",
        "        result_each = {}\n",
        "        result_each['epochs_'] = epochs_\n",
        "        result_each['k_'] = k_\n",
        "        result_each['d_'] = d_\n",
        "        result_each['mean_useful'] = mean_useful\n",
        "        result_each['std_dev_useful'] = std_dev_useful.tolist()\n",
        "        result_each['mean_non_useful'] = mean_non_useful\n",
        "        result_each['std_dev_non_useful'] = std_dev_non_useful.tolist()\n",
        "        result_each['class_count'] = class_count\n",
        "        result_each['n_nodes'] = n_nodes\n",
        "        result_each['lr'] = lr\n",
        "        result_each['before'] = before\n",
        "        result_each['after'] = after\n",
        "        result_each['loss_array'] = loss_array.tolist()[::50]\n",
        "        result_each['acc_array'] = acc_array.tolist()[::50]\n",
        "        result_each['attention_loss_array'] = attention_loss_array.tolist()[::50]\n",
        "        result_each['test_accuracy'] = test_accuracy\n",
        "        \n",
        "        result.append(result_each)\n",
        "        \n",
        "        cnt += 1\n",
        "        print(\"Experiment: \" + str(cnt) + \" completed\")\n",
        "      "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-cd224622a4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mn_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_useful_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_useful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_dev_useful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_non_useful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_dev_non_useful\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_useful_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 2)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Rrrqh9XKDYwp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.scatter([1,2,3])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frwX5LcJ-MiX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6c7bda13-52eb-4829-93e3-4bb09beb4ac6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531908585370,
          "user_tz": -330,
          "elapsed": 1114,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'epochs_': 5000, 'k_': 2, 'd_': 1, 'mean_useful': [0], 'std_dev_useful': [[0.7484104447742972]], 'mean_non_useful': [2], 'std_dev_non_useful': [[-0.6543975649538246]], 'class_count': {'0': 1500, '1': 0}, 'n_nodes': 2, 'lr': 0.1, 'before': True, 'after': True, 'loss_array': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'acc_array': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'attention_loss_array': [723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56, 723.56], 'test_accuracy': 1.0}, {'epochs_': 5000, 'k_': 25, 'd_': 2, 'mean_useful': [0, 0], 'std_dev_useful': [[-0.37103607063697125, 0.8576389779686328], [0.9044465636835899, 0.8251550241979588]], 'mean_non_useful': [2, 2], 'std_dev_non_useful': [[0.48611667173126094, -0.29078533483141433], [-0.5155746169307235, -0.5330144451355975]], 'class_count': {'0': 411, '1': 1089}, 'n_nodes': 2, 'lr': 0.1, 'before': True, 'after': True, 'loss_array': [0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322, 0.7263681888580322], 'acc_array': [0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199, 0.2736318407960199], 'attention_loss_array': [3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6, 3235.6], 'test_accuracy': 0.27474747474747474}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsVuuJhO6o1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Result as JSON"
      ]
    },
    {
      "metadata": {
        "id": "KOUYR1g3pZA1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('result3.json', 'w') as fp:\n",
        "    \n",
        "    json.dump(result, fp)\n",
        "    \n",
        "from google.colab import files\n",
        "files.download('result3.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CMgo9jUX2rHB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}