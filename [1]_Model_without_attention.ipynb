{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[1] Model without attention.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9Tfb-_l7ivwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "metadata": {
        "id": "zuGbVGVQiJqe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3f92dab-34f0-406f-e65e-a098f5b083a5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531918376718,
          "user_tz": -330,
          "elapsed": 3806,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rBFOKilPjFAe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aO-MjsH61AA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epochs_ = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_b4-HiIkivN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ]
    },
    {
      "metadata": {
        "id": "mOLK_f66kp0f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_data(k=2, d=1, k_useful=1, n_rows=1500, print_param=False):\n",
        "  \n",
        "  \"\"\"\n",
        "  generate data that is suitable for attention models with no dependency between parts of the image \n",
        "  \n",
        "  :parameters\n",
        "  k - number of parts in each data point\n",
        "  d - number of sub parts in each 'k' part\n",
        "  k_useful - number of parts with useful information [k_useful <= k]\n",
        "  n_rows - number of rows in the data set\n",
        "  print_param - boolean value to set the printinf status of parametrs\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  \n",
        "  loc_useful = 0\n",
        "  scale_useful = 1\n",
        "  loc_non_useful = 2\n",
        "  scale_non_useful = 1\n",
        "  \n",
        "  is_useful_all = []\n",
        "  \n",
        "  for _ in range(n_rows):\n",
        "\n",
        "    # each element of 'is_useful' shows whether the corresponding part is useful or not\n",
        "    is_useful = np.array([0]*k)\n",
        "    useful_idx = np.random.choice(range(k), size=k_useful, replace=False)\n",
        "    for i in useful_idx:\n",
        "      is_useful[i] = 1\n",
        "    is_useful_all.append(list(is_useful))\n",
        "\n",
        "    data_point = []\n",
        "    for i in range(k):\n",
        "      data_part = []\n",
        "      if is_useful[i] == 1:\n",
        "        data_part = np.random.normal(loc=loc_useful, scale=scale_useful, size=d)\n",
        "      else:\n",
        "        #loc_non_useful = np.random.rand()\n",
        "        #scale_non_useful = np.random.choice(np.linspace(0, 100, 10))\n",
        "        data_part = np.random.normal(loc=loc_non_useful, scale=scale_non_useful, size=d)\n",
        "      data_point.append(list(data_part))\n",
        "    \n",
        "    data.append(data_point)\n",
        "    \n",
        "  if print_param:\n",
        "    print(\"number of parts: \", k)\n",
        "    print(\"useful parts: \", is_useful_all[:5])\n",
        "    print(\"loc_non_useful: \", loc_useful)\n",
        "    print(\"scale_non_useful: \", scale_useful)\n",
        "    print(\"loc_non_useful: \", loc_non_useful)\n",
        "    print(\"scale_non_useful: \", scale_non_useful)\n",
        "    print(\"-\"*50)\n",
        "                             \n",
        "  data = np.array(data)\n",
        "  data = data.round(decimals=2)\n",
        "  return data, is_useful_all, loc_useful, np.array([scale_useful]), loc_non_useful, np.array([scale_non_useful])                            \n",
        "                                  \n",
        "#data, is_useful_all = generate_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1vnS_jMl1eu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_data_2(k=2, d=1, k_useful=1, n_rows=1500, print_param=False):\n",
        "  \n",
        "  \"\"\"\n",
        "  generate data suitable for attention models with the dependency between parts of the image\n",
        "  \n",
        "  :parameters\n",
        "  k - number of parts in each data point\n",
        "  d - number of sub parts in each 'k' part\n",
        "  k_useful - number of parts with useful information [k_useful <= k]\n",
        "  n_rows - number of rows in the data set\n",
        "  print_param - boolean value to set the printinf status of parametrs\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  \n",
        "  mean_useful = [0]*d_\n",
        "  std_dev_useful = np.abs(np.random.randn(d_, d_))\n",
        "  mean_non_useful = [20]*d_\n",
        "  std_dev_non_useful = np.abs(np.random.randn(d_, d_))\n",
        "  \n",
        "  is_useful_all = []\n",
        "  \n",
        "  for _ in range(n_rows):\n",
        "\n",
        "    # each element of 'is_useful' shows whether the corresponding part is useful or not\n",
        "    is_useful = np.array([0]*k)\n",
        "    useful_idx = np.random.choice(range(k), size=k_useful, replace=False)\n",
        "    for i in useful_idx:\n",
        "      is_useful[i] = 1\n",
        "    is_useful_all.append(list(is_useful))\n",
        "\n",
        "    data_point = []\n",
        "    for i in range(k):\n",
        "      data_part = []\n",
        "      if is_useful[i] == 1:\n",
        "        data_part = np.matmul(np.random.randn(d_), std_dev_useful) + mean_useful\n",
        "        #data_part = np.random.normal(loc=loc_useful, scale=scale_useful, size=d)\n",
        "      else:\n",
        "        data_part = np.matmul(np.random.randn(d_), std_dev_non_useful) + mean_non_useful\n",
        "        #loc_non_useful = np.random.rand()\n",
        "        #scale_non_useful = np.random.choice(np.linspace(0, 100, 10))\n",
        "        #data_part = np.random.normal(loc=loc_non_useful, scale=scale_non_useful, size=d)\n",
        "      data_point.append(list(data_part))\n",
        "    \n",
        "    data.append(data_point)\n",
        "    \n",
        "  if print_param:\n",
        "    print(\"number of parts: \", k)\n",
        "    print(\"useful parts: \", is_useful_all[:5])\n",
        "    print(\"mean_non_useful: \", mean_useful)\n",
        "    print(\"std_dev_non_useful: \", std_dev_useful)\n",
        "    print(\"mean_non_useful: \", mean_non_useful)\n",
        "    print(\"std_dev_non_useful: \", std_dev_non_useful)\n",
        "    print(\"-\"*50)\n",
        "                             \n",
        "  data = np.array(data)\n",
        "  data = data.round(decimals=2)\n",
        "  return data, is_useful_all, mean_useful, std_dev_useful, mean_non_useful, std_dev_non_useful \n",
        "                                     \n",
        "                                  \n",
        "#data, is_useful_all = generate_data_2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dEQn5cVnjNc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def find_y(x, is_useful_all):\n",
        "  \n",
        "  \"\"\"\n",
        "  find 'y' (class) based on the useful part\n",
        "  \n",
        "  :parameters\n",
        "  x - data features\n",
        "  is_useful_all - boolean nxk array\n",
        "  d - number of sub parts in each 'k' part of each data point\n",
        "  \"\"\"\n",
        "  x_useful = []\n",
        "  d = x.shape[2]\n",
        "  \n",
        "  for row, is_useful in zip(x, is_useful_all):\n",
        "    useful_sum = np.array([0.]*d)\n",
        "    for idx, value in enumerate(is_useful):\n",
        "      if value == 1:\n",
        "        useful_sum += row[idx]\n",
        "    x_useful.append(useful_sum)\n",
        "    \n",
        "  x_useful = np.array(x_useful)\n",
        "  w = np.random.uniform(low=-1, high=1, size=(d, 1))\n",
        "  #print(x_useful.shape, w.shape)\n",
        "  # n_rows*d, d*1\n",
        "  # print(x_useful, w)\n",
        "  y = np.matmul(x_useful, w)\n",
        "  y = (y>0).astype(int)\n",
        "  return y\n",
        "\n",
        "#y = find_y(data, is_useful_all)\n",
        "#print(y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdCL-VjX311i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_useful_data(data, is_useful_all):\n",
        "  '''\n",
        "  Warning: Works only when n_useful=1\n",
        "  '''\n",
        "  data_useful = []\n",
        "\n",
        "  for row, is_useful in zip(data, is_useful_all):\n",
        "    for idx, value in enumerate(is_useful):\n",
        "      if value == 1:\n",
        "        data_useful.append(row[idx])\n",
        "        break\n",
        "\n",
        "  data_useful = np.array(data_useful)\n",
        "  \n",
        "  return data_useful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KmDNDMj2ZuB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def display_plot(data, is_useful_all, y_pred_int, y_int, plot_type):\n",
        "  \n",
        "  '''\n",
        "  plot_type: \n",
        "  0 (special_case - all [2d]), \n",
        "  1 (special_case - useful [1d]), \n",
        "  2 (not_special_case - useful [2d])\n",
        "  '''\n",
        "  \n",
        "  f1 = None\n",
        "  f2 = None\n",
        "  if plot_type == 0:\n",
        "    f1 = data.reshape(-1,2)[:,0]\n",
        "    f2 = data.reshape(-1,2)[:,0]\n",
        "  elif plot_type == 1:\n",
        "    data_useful = get_useful_data(data, is_useful_all)\n",
        "    #print(data_useful.shape)\n",
        "    f1 = data_useful[:,0]\n",
        "    f2 = np.ones(len(f1))\n",
        "  else:\n",
        "    data_useful = get_useful_data(data, is_useful_all)\n",
        "    f1 = data_useful[:,0]\n",
        "    f2 = data_useful[:,1]\n",
        "    \n",
        "  #print(f1.shape)\n",
        "  #print(f2.shape)\n",
        "  \n",
        "  plt.figure(figsize=(10,5))\n",
        "  \n",
        "  plt.subplot(121)\n",
        "  plt.scatter(f1, f2, c=y_pred_int, label=y_pred_int, s=3, cmap='RdYlGn', alpha=0.5)\n",
        "  plt.xlabel(\"f1\")\n",
        "  plt.ylabel(\"f2\")\n",
        "  plt.title(\"[Predicted] Epoch \" + str(i))\n",
        "  #plt.legend()\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.scatter(f1, f2, c=y_int, label=y_int, s=3, cmap='RdYlGn', alpha=0.5)\n",
        "  plt.xlabel(\"f1\")\n",
        "  plt.ylabel(\"f2\")\n",
        "  plt.title(\"[Real] Epoch \" + str(i))\n",
        "  #plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdU43Rgqy-ST",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "CQS6ggYqy9RU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# both before and after\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model, self).__init__()\n",
        "    #n_nodes = 2\n",
        "    ip_n = k_*d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = n_nodes\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(hmid_n, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, hmid_n)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "    '''\n",
        "    print(\"Weights\")\n",
        "    print(self.h1_layer.weight.size())\n",
        "    print(self.h1_layer.weight.data)\n",
        "    print(self.h2_layer.weight.data)\n",
        "    print(self.op_layer.weight.data)\n",
        "    '''\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    '''\n",
        "    print(\"Bias\")\n",
        "    print(self.h1_layer.bias.size())\n",
        "    print(self.h1_layer.bias.data)\n",
        "    print(self.h2_layer.bias.data)\n",
        "    print(self.op_layer.bias.data)\n",
        "    '''\n",
        "\n",
        "\n",
        "  def forward(self, x, before, after):\n",
        "    \n",
        "    y_pred=None\n",
        "    \n",
        "    #print(x.shape)\n",
        "    x = x.view(-1, k_*d_)\n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    #print(op_h1_layer.shape)\n",
        "    \n",
        "    if before and after:\n",
        "      op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      op_h2_layer = self.relu(self.h2_layer(op_hmid_layer))\n",
        "      y_pred = self.relu(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    elif before:\n",
        "      op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      #op_h2_layer = self.relu(self.h2_layer(op_h1_layer))\n",
        "      y_pred = self.relu(self.op_layer(op_hmid_layer))\n",
        "      \n",
        "    elif after:\n",
        "      #op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      op_h2_layer = self.relu(self.h2_layer(op_h1_layer))\n",
        "      y_pred = self.relu(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    else:\n",
        "      #op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      #op_h2_layer = self.relu(self.h2_layer(op_h1_layer))\n",
        "      y_pred = self.relu(self.op_layer(op_h1_layer))\n",
        "      \n",
        "    return y_pred\n",
        "    \n",
        "#model = Model(n_nodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OhwS8aGHjb4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ]
    },
    {
      "metadata": {
        "id": "MGSxXVRljYf3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "76cff82e-9cb2-4b34-ede1-70ad7cd1e6cb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531918420346,
          "user_tz": -330,
          "elapsed": 20461,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "result = []\n",
        "cnt = 0\n",
        "\n",
        "k_d_list = [(2,1), (25,2), (25,100)] # list of tuples in (k, d) format\n",
        "for k_, d_ in k_d_list[:2]: ##I1\n",
        "  #result[str((k_, d_))] = {}\n",
        "  is_special_case = (k_==2 and d_==1)\n",
        "  \n",
        "  n_ = max(k_*d_*30, 1500)\n",
        "  #n_ = 10\n",
        "  \n",
        "  data, is_useful_all, mean_useful, std_dev_useful, mean_non_useful, std_dev_non_useful = generate_data_2(k=k_, d=d_, n_rows=n_)\n",
        "  y = find_y(data, is_useful_all)\n",
        "  #print(is_useful_all)\n",
        "  \n",
        "  x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split(data, y, range(n_), test_size=0.33, random_state=100, stratify=y)\n",
        "  #print(idx_train)\n",
        "  \n",
        "  x_train_tensor = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "  x_test_tensor = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "  y_train_tensor = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "  y_test_tensor = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "  \n",
        "  '''\n",
        "  print(data.shape)\n",
        "  print(X_train.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_train.shape)\n",
        "  print(y_test.shape)\n",
        "  input()\n",
        "  '''\n",
        "  \n",
        "  n_nodes_list = [2, 4, 8, 16] # number of nodes in every layer\n",
        "  for n_nodes in n_nodes_list[:1]: ##I2\n",
        "    #result[str((k_, d_))][str(n_nodes)] = {}\n",
        "    \n",
        "    model = Model(n_nodes=n_nodes)\n",
        "    loss_method = torch.nn.MSELoss()\n",
        "    \n",
        "    lr_list = [0.1, 0.01, 0.001, 0.0001]\n",
        "    for lr in lr_list[:1]: ## I3\n",
        "      #result[str((k_, d_))][str(n_nodes)][str(lr)] = {}\n",
        "      \n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "      \n",
        "      before_after_list = [[True, True], [True, False], [False, True], [False, False]]\n",
        "      for before, after in before_after_list[:1]: #I4\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))] = {}\n",
        "      \n",
        "        # go forward and backward over the network and update parameters\n",
        "\n",
        "        display_steps = np.linspace(0,epochs_, num=5, dtype=int)\n",
        "        display_steps = np.append(display_steps, [epochs_-1])\n",
        "\n",
        "        loss_array = np.array([])\n",
        "        acc_array = np.array([])\n",
        "\n",
        "        for i in range(epochs_):\n",
        "            \n",
        "            y_train_pred = model.forward(x_train_tensor, before=before, after=after)\n",
        "\n",
        "            loss = loss_method(y_train_pred, y_train_tensor)\n",
        "            loss_array = np.append(loss_array, [loss.item()])\n",
        "\n",
        "            y_train_pred_int = (y_train_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "            y_train_int = y_train_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "            accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_train_pred_int, y_train_int)])/len(y_train_tensor)\n",
        "            acc_array = np.append(acc_array, [accuracy])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            '''\n",
        "            is_useful_all = np.array(is_useful_all)\n",
        "            if i in display_steps:\n",
        "              # visualize training\n",
        "              if not is_special_case:\n",
        "                display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=2)\n",
        "              else:\n",
        "                #display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=0)\n",
        "                display_plot(x_train, is_useful_all[idx_train], y_train_pred_int, y_train_int, plot_type=1)\n",
        "            '''\n",
        "         \n",
        "        y_test_pred = model.forward(x_test_tensor, before=before, after=after)\n",
        "        y_test_pred_int = (y_test_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "        y_test_int = y_test_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "        test_accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_test_pred_int, y_test_int)])/len(y_test_tensor)\n",
        "        \n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['loss_array'] = loss_array.tolist()\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['acc_array'] = acc_array.tolist()\n",
        "        #result[str((k_, d_))][str(n_nodes)][str(lr)][str((before, after))]['test_accuracy'] = test_accuracy\n",
        "        \n",
        "        class_count = {}\n",
        "        class_count['0'] = y.flatten().tolist().count(0)\n",
        "        class_count['1'] = y.flatten().tolist().count(1)\n",
        "        \n",
        "        result_each = {}\n",
        "        result_each['epochs_'] = epochs_\n",
        "        result_each['k_'] = k_\n",
        "        result_each['d_'] = d_\n",
        "        result_each['mean_useful'] = mean_useful\n",
        "        result_each['std_dev_useful'] = std_dev_useful.tolist()\n",
        "        result_each['mean_non_useful'] = mean_non_useful\n",
        "        result_each['std_dev_non_useful'] = std_dev_non_useful.tolist()\n",
        "        result_each['class_count'] = class_count\n",
        "        result_each['n_nodes'] = n_nodes\n",
        "        result_each['lr'] = lr\n",
        "        result_each['before'] = before\n",
        "        result_each['after'] = after\n",
        "        result_each['loss_array'] = loss_array.tolist()[::50]\n",
        "        result_each['acc_array'] = acc_array.tolist()[::50]\n",
        "        #result_each['attention_loss_array'] = attention_loss_array.tolist()[::50]\n",
        "        result_each['test_accuracy'] = test_accuracy\n",
        "        \n",
        "        result.append(result_each)\n",
        "        \n",
        "        cnt += 1\n",
        "        print(\"Experiment: \" + str(cnt) + \" completed\")\n",
        "      "
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment: 1 completed\n",
            "Experiment: 2 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frwX5LcJ-MiX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "3420ed05-4638-455c-d544-7159b38c3361",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531918427145,
          "user_tz": -330,
          "elapsed": 1011,
          "user": {
            "displayName": "Gokul Karthik",
            "photoUrl": "//lh4.googleusercontent.com/-PJ7R6mdE_fs/AAAAAAAAAAI/AAAAAAAABxQ/UjdaP8x8Tj8/s50-c-k-no/photo.jpg",
            "userId": "113156225842877407217"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(result[0]['mean_useful'])\n",
        "print(result[0]['std_dev_useful'])\n",
        "print(result[0]['mean_non_useful'])\n",
        "print(result[0]['std_dev_non_useful'])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[[0.8044583035248052]]\n",
            "[20]\n",
            "[[0.3209315470898572]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nsVuuJhO6o1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Result as JSON"
      ]
    },
    {
      "metadata": {
        "id": "KOUYR1g3pZA1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('result1.json', 'w') as fp:\n",
        "    \n",
        "    json.dump(result, fp)\n",
        "    \n",
        "from google.colab import files\n",
        "files.download('result1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}