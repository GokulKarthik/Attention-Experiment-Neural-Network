{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[1]ModelWithoutAttention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8-p9dmEX6ZJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "metadata": {
        "id": "x4S-RlXT6OU5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nYjrv-1V6WMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "np.random.seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "glu_pqEv6bBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs_ = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRyAALym8o0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "22EFH-LJFllR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BNurXSX5F_kb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guUrlkbJIlyC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data = None\n",
        "\n",
        "with open('data.json') as f:\n",
        "    all_data = json.load(f)\n",
        "\n",
        "print(type(all_data))\n",
        "print(len(all_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JUU9VDAHNxV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "Xh8NiI9dGAZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    \n",
        "    super(Model, self).__init__()\n",
        "    ip_n = k_*d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = n_nodes\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(hmid_n, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, hmid_n)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    \n",
        "    \n",
        "  def forward(self, x, before, after):\n",
        "    \n",
        "    y_pred=None\n",
        "    x = x.view(-1, k_*d_)\n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    \n",
        "    if before and after:\n",
        "      op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      op_h2_layer = self.relu(self.h2_layer(op_hmid_layer))\n",
        "      y_pred = self.sigmoid(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    elif before:\n",
        "      op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "      y_pred = self.sigmoid(self.op_layer(op_hmid_layer))\n",
        "      \n",
        "    elif after:\n",
        "      op_h2_layer = self.relu(self.h2_layer(op_h1_layer))\n",
        "      y_pred = self.sigmoid(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    else:\n",
        "      y_pred = self.sigmoid(self.op_layer(op_h1_layer))\n",
        "      \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMEDBjfZIEgN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ]
    },
    {
      "metadata": {
        "id": "6wNX72x9KRf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_loss(y, y_pred):\n",
        "  return np.round(np.sum(-y*np.log(y_pred + 1e-8)), decimals=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xYhMRv3XIGMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result = []\n",
        "cnt = 0\n",
        "\n",
        "for data in all_data: ##I1\n",
        "  \n",
        "  x_train = np.array(data['x_train'])\n",
        "  x_test = np.array(data['x_test'])\n",
        "  y_train = np.array(data['y_train'])\n",
        "  y_test = np.array(data['y_test'])\n",
        "  w = np.array(data['w'])\n",
        "  is_useful_all = np.array(data['is_useful_all'])\n",
        "  idx_train = np.array(data['idx_train'])\n",
        "  idx_test = np.array(data['idx_test'])\n",
        "  \n",
        "  k_, d_ = x_train.shape[1], x_train.shape[2]\n",
        "  \n",
        "  x_train_tensor = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "  x_test_tensor = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "  y_train_tensor = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "  y_test_tensor = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "  \n",
        "  n_nodes_list = [2, 4, 8, 16] # number of nodes in every layer\n",
        "  for n_nodes in n_nodes_list[:]: ##I2\n",
        "    \n",
        "    model = Model(n_nodes=n_nodes)\n",
        "    loss_method = torch.nn.BCELoss()\n",
        "    \n",
        "    lr_list = [0.1, 0.01, 0.001, 0.0001]\n",
        "    for lr in lr_list[:]: ## I3\n",
        "      \n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "      \n",
        "      before_after_list = [[True, True], [True, False], [False, True], [False, False]]\n",
        "      for before, after in before_after_list[:]: #I4\n",
        "      \n",
        "        # go forward and backward over the network and update parameters\n",
        "\n",
        "        loss_array = np.array([])\n",
        "        acc_array = np.array([])\n",
        "        \n",
        "        initial_weight = {}\n",
        "        initial_bias = {}\n",
        "        final_weight = {}\n",
        "        final_bias = {}\n",
        "\n",
        "        for i in range(epochs_):\n",
        "\n",
        "          y_train_pred = model.forward(x_train_tensor, before=before, after=after)\n",
        "\n",
        "          if i==0:\n",
        "\n",
        "            if before and after:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif before:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif after:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            else:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "          if i==epochs_-1:\n",
        "\n",
        "            if before and after:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif before:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif after:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            else:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "          loss = loss_method(y_train_pred, y_train_tensor)\n",
        "          loss_array = np.append(loss_array, [loss.item()])\n",
        "\n",
        "          y_train_pred_int = (y_train_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "          y_train_int = y_train_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "          accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_train_pred_int, y_train_int)])/len(y_train_tensor)\n",
        "          acc_array = np.append(acc_array, [accuracy])\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()    \n",
        "         \n",
        "        y_test_pred = model.forward(x_test_tensor, before=before, after=after)\n",
        "        y_test_pred_int = (y_test_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "        y_test_int = y_test_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "        test_accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_test_pred_int, y_test_int)])/len(y_test_tensor)\n",
        "        test_loss = log_loss(y_test_int, y_test_pred_int)\n",
        "        \n",
        "        result_each = {}\n",
        "        result_each['epochs_'] = epochs_\n",
        "        result_each['k_'] = k_\n",
        "        result_each['d_'] = d_\n",
        "        result_each['n_nodes'] = n_nodes\n",
        "        result_each['lr'] = lr\n",
        "        result_each['before'] = before\n",
        "        result_each['after'] = after\n",
        "        result_each['loss_array'] = loss_array.tolist()[::50]\n",
        "        result_each['acc_array'] = acc_array.tolist()[::50]\n",
        "        result_each['initial_weight'] = initial_weight\n",
        "        result_each['initial_bias'] = initial_bias\n",
        "        result_each['final_weight'] = final_weight\n",
        "        result_each['final_bias'] = final_bias\n",
        "        result_each['test_accuracy'] = test_accuracy\n",
        "        result_each['test_loss'] = test_loss\n",
        "        \n",
        "        result.append(result_each)\n",
        "        \n",
        "        cnt += 1\n",
        "        print(\"Experiment: \" + str(cnt) + \" completed\")\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRjB8y_WAKEe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('result1.json', 'w') as fp:\n",
        "    json.dump(result, fp)   \n",
        "    \n",
        "from google.colab import files\n",
        "files.download('result1.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q72SQylYX7zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}