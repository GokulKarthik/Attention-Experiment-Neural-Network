{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[3]ModelWithAttentionAndAggregation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w-I63tx5Bf_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "metadata": {
        "id": "RWVPEE8uAz4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torch -U"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3XKg7RgBnmq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "np.random.seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EGUIGAyBqU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs_ = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7y5F3xvxBslA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "FjbD_S5OBxKN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cfm0KouGB01q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lR9HyM3aB3Fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_data = None\n",
        "\n",
        "with open('data.json') as f:\n",
        "    all_data = json.load(f)\n",
        "\n",
        "print(type(all_data))\n",
        "print(len(all_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5ST03zxB5O2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "RtmKwgUyB5-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# both before and after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model1(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model1, self).__init__()\n",
        "    ip_n = d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    \n",
        "    \n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "\n",
        "    attention = x*op_softmax\n",
        "    attention_new = attention.sum(1)\n",
        "    op_h2_layer = self.relu(self.h2_layer(attention_new))\n",
        "    y_pred = self.sigmoid(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    return attention_capture, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yZjVpGJLB-pv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# only before\n",
        "softmax_res = [] \n",
        "\n",
        "class Model2(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model2, self).__init__()\n",
        "    ip_n = d_\n",
        "    h1_n = n_nodes\n",
        "    hmid_n = 1\n",
        "    op_n = 1\n",
        "    self.h1_layer = torch.nn.Linear(ip_n, h1_n)\n",
        "    self.hmid_layer = torch.nn.Linear(h1_n, hmid_n)\n",
        "    self.op_layer = torch.nn.Linear(d_, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    # deviation\n",
        "    h1_stdv = 1./np.sqrt(h1_n)\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.h1_layer.weight.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=(h1_n, ip_n)))\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, h1_n)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, d_)))\n",
        "\n",
        "    # bias init\n",
        "    self.h1_layer.bias.data = torch.Tensor(np.random.uniform(low=-h1_stdv, high=h1_stdv, size=h1_n))\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_h1_layer = self.relu(self.h1_layer(x))\n",
        "    op_hmid_layer = self.relu(self.hmid_layer(op_h1_layer))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    \n",
        "    attention = x*op_softmax\n",
        "    attention_new = attention.sum(1) \n",
        "    y_pred = self.sigmoid(self.op_layer(attention_new))\n",
        "      \n",
        "    return attention_capture, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgr67P0qCC1N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# only after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model3(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model3, self).__init__()\n",
        "    ip_n = d_\n",
        "    hmid_n = 1\n",
        "    h2_n = n_nodes\n",
        "    op_n = 1\n",
        "    self.hmid_layer = torch.nn.Linear(ip_n, hmid_n)\n",
        "    self.h2_layer = torch.nn.Linear(d_, h2_n)\n",
        "    self.op_layer = torch.nn.Linear(h2_n, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    # deviation\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    h2_stdv = 1./np.sqrt(h2_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, ip_n)))\n",
        "    self.h2_layer.weight.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=(h2_n, d_)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, h2_n)))\n",
        "\n",
        "    # bias init\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.h2_layer.bias.data = torch.Tensor(np.random.uniform(low=-h2_stdv, high=h2_stdv, size=h2_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_hmid_layer = self.relu(self.hmid_layer(x))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    \n",
        "   \n",
        "    attention = x*op_softmax\n",
        "    attention_new = attention.sum(1)\n",
        "    op_h2_layer = self.relu(self.h2_layer(attention_new))\n",
        "    y_pred = self.sigmoid(self.op_layer(op_h2_layer))\n",
        "      \n",
        "    return attention_capture, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCyEZhq4CFSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the structure of NN\n",
        "# no before and after\n",
        "softmax_res = [] \n",
        "\n",
        "class Model4(torch.nn.Module):\n",
        "        \n",
        "  def __init__(self, n_nodes):\n",
        "    super(Model4, self).__init__()\n",
        "    ip_n = d_\n",
        "    hmid_n = 1\n",
        "    op_n = 1\n",
        "    self.hmid_layer = torch.nn.Linear(ip_n, hmid_n)\n",
        "    self.op_layer = torch.nn.Linear(d_, op_n)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    # deviation\n",
        "    hmid_stdv = 1./np.sqrt(hmid_n)\n",
        "    op_stdv = 1./np.sqrt(op_n)\n",
        "\n",
        "    # weight init\n",
        "    self.hmid_layer.weight.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=(hmid_n, ip_n)))\n",
        "    self.op_layer.weight.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=(op_n, d_)))\n",
        "\n",
        "    # bias init\n",
        "    self.hmid_layer.bias.data = torch.Tensor(np.random.uniform(low=-hmid_stdv, high=hmid_stdv, size=hmid_n))\n",
        "    self.op_layer.bias.data = torch.Tensor(np.random.uniform(low=-op_stdv, high=op_stdv, size=op_n))\n",
        "\n",
        "  def forward(self, x):\n",
        "  \n",
        "    op_hmid_layer = self.relu(self.hmid_layer(x))\n",
        "    op_softmax = F.softmax(op_hmid_layer, dim=1)\n",
        "    \n",
        "    attention_capture = op_softmax.squeeze()\n",
        "    attention_capture = attention_capture.detach().numpy()\n",
        "    \n",
        "    attention = x*op_softmax\n",
        "    attention_new = attention.sum(1)\n",
        "    y_pred = self.sigmoid(self.op_layer(attention_new))\n",
        "      \n",
        "    return attention_capture, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uTeAc3E7CIIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ]
    },
    {
      "metadata": {
        "id": "dbMQ0oeICIxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_loss(y, y_pred):\n",
        "  return np.round(np.sum(-y*np.log(y_pred + 1e-8)), decimals=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvKKG-PMMsM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_attention_accuracy(is_useful, attention_capture):\n",
        "  return sum(np.argmax(is_useful, axis=1) == np.argmax(attention_capture, axis=1))/is_useful.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SicWvRb-CPlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result = []\n",
        "cnt = 0\n",
        "attention_capture = None\n",
        "\n",
        "for data in all_data: ##I1\n",
        "  \n",
        "  x_train = np.array(data['x_train'])\n",
        "  x_test = np.array(data['x_test'])\n",
        "  y_train = np.array(data['y_train'])\n",
        "  y_test = np.array(data['y_test'])\n",
        "  w = np.array(data['w'])\n",
        "  is_useful_all = np.array(data['is_useful_all'])\n",
        "  idx_train = np.array(data['idx_train'])\n",
        "  idx_test = np.array(data['idx_test'])\n",
        "  \n",
        "  k_, d_ = x_train.shape[1], x_train.shape[2]\n",
        "  \n",
        "  x_train_tensor = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
        "  x_test_tensor = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
        "  y_train_tensor = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "  y_test_tensor = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "  \n",
        "  is_useful_train = np.array(is_useful_all)[idx_train]\n",
        "  is_useful_test = np.array(is_useful_all)[idx_test]\n",
        "  \n",
        "  n_nodes_list = [2, 4, 8, 16] # number of nodes in every layer\n",
        "  for n_nodes in n_nodes_list[:]: ##I2\n",
        "    \n",
        "    lr_list = [0.1, 0.01, 0.001, 0.0001]\n",
        "    for lr in lr_list[:]: ## I3\n",
        "      \n",
        "      before_after_list = [[True, True], [True, False], [False, True], [False, False]]\n",
        "      for before, after in before_after_list[:]: #I4\n",
        "        \n",
        "        if before and after:\n",
        "          model = Model1(n_nodes=n_nodes)\n",
        "        elif before:\n",
        "          model = Model2(n_nodes=n_nodes)\n",
        "        elif after:\n",
        "          model = Model3(n_nodes=n_nodes)\n",
        "        else:\n",
        "          model = Model4(n_nodes=n_nodes)\n",
        "          \n",
        "        loss_method = torch.nn.BCELoss()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "      \n",
        "        # go forward and backward over the network and update parameters\n",
        "\n",
        "        loss_array = np.array([])\n",
        "        acc_array = np.array([])\n",
        "        attention_accuracy_array = np.array([])\n",
        "        attention_loss_array = np.array([])\n",
        "        \n",
        "        initial_weight = {}\n",
        "        initial_bias = {}\n",
        "        final_weight = {}\n",
        "        final_bias = {}\n",
        "\n",
        "        for i in range(epochs_):\n",
        "            \n",
        "          attention_capture, y_train_pred = model.forward(x_train_tensor)\n",
        "\n",
        "          if i==0:\n",
        "\n",
        "            if before and after:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif before:\n",
        "              initial_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif after:\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            else:\n",
        "              initial_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              initial_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              initial_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              initial_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "          if i==epochs_-1:\n",
        "\n",
        "            if before and after:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif before:\n",
        "              final_weight['h1_layer'] = model.h1_layer.weight.data.numpy().tolist()\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['h1_layer'] = model.h1_layer.bias.data.numpy().tolist()\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            elif after:\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['h2_layer'] = model.h2_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['h2_layer'] = model.h2_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "            else:\n",
        "              final_weight['hmid_layer'] = model.hmid_layer.weight.data.numpy().tolist()\n",
        "              final_weight['op_layer'] = model.op_layer.weight.data.numpy().tolist()\n",
        "\n",
        "              final_bias['hmid_layer'] = model.hmid_layer.bias.data.numpy().tolist()\n",
        "              final_bias['op_layer'] = model.op_layer.bias.data.numpy().tolist()\n",
        "\n",
        "          attention_accuracy = find_attention_accuracy(is_useful_train, attention_capture)\n",
        "          attention_accuracy_array = np.append(attention_accuracy_array, [attention_accuracy])\n",
        "          \n",
        "          attention_loss = log_loss(is_useful_train, attention_capture)\n",
        "          attention_loss_array = np.append(attention_loss_array, [attention_loss])\n",
        "\n",
        "          loss = loss_method(y_train_pred, y_train_tensor)\n",
        "          loss_array = np.append(loss_array, [loss.item()])\n",
        "\n",
        "          y_train_pred_int = (y_train_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "          y_train_int = y_train_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "          accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_train_pred_int, y_train_int)])/len(y_train_tensor)\n",
        "          acc_array = np.append(acc_array, [accuracy])\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "         \n",
        "        attention_capture_test, y_test_pred = model.forward(x_test_tensor)\n",
        "        y_test_pred_int = (y_test_pred>=0.5).squeeze().type(torch.IntTensor).data.numpy()\n",
        "        y_test_int = y_test_tensor.squeeze().type(torch.IntTensor).data.numpy()\n",
        "        \n",
        "        test_accuracy = sum([int(v1 == v2) for v1, v2 in zip(y_test_pred_int, y_test_int)])/len(y_test_tensor)\n",
        "        test_loss = log_loss(y_test_int, y_test_pred_int)\n",
        "        \n",
        "        test_attention_accuracy = find_attention_accuracy(is_useful_test, attention_capture_test)\n",
        "        test_attention_loss = log_loss(is_useful_test, attention_capture_test)\n",
        "    \n",
        "        result_each = {}\n",
        "        result_each['epochs_'] = epochs_\n",
        "        result_each['k_'] = k_\n",
        "        result_each['d_'] = d_\n",
        "        result_each['n_nodes'] = n_nodes\n",
        "        result_each['lr'] = lr\n",
        "        result_each['before'] = before\n",
        "        result_each['after'] = after\n",
        "        result_each['loss_array'] = loss_array.tolist()[::50]\n",
        "        result_each['acc_array'] = acc_array.tolist()[::50]\n",
        "        result_each['attention_accuracy_array'] = attention_accuracy_array.tolist()[::50]\n",
        "        result_each['attention_loss_array'] = attention_loss_array.tolist()[::50]\n",
        "        result_each['initial_weight'] = initial_weight\n",
        "        result_each['initial_bias'] = initial_bias\n",
        "        result_each['final_weight'] = final_weight\n",
        "        result_each['final_bias'] = final_bias\n",
        "        result_each['test_accuracy'] = test_accuracy\n",
        "        result_each['test_loss'] = test_loss\n",
        "        result_each['test_attention_accuracy'] = test_attention_accuracy\n",
        "        result_each['test_attention_loss'] = test_attention_loss\n",
        "        \n",
        "        result.append(result_each)\n",
        "        \n",
        "        cnt += 1\n",
        "        print(\"Experiment: \" + str(cnt) + \" completed\")\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RpQmitj6FDWv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('result3.json', 'w') as fp:\n",
        "    json.dump(result, fp)   \n",
        "    \n",
        "from google.colab import files\n",
        "files.download('result3.json')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}