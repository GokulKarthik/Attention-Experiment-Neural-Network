# Experimental understanding of attention models in Deep Learning
Attention based neural network models have produced more accurate results in recent years though there is no concrete theoretical proof on “Why attention works?”. Hence we experimented the models with and without attention under certain assumptions and tried to understand “How those models learn?”

We generated the proxy data for images with differentiated statistical properties for ’background’ and ’foreground’ regions. The trace of detecting this ’foreground’ region in the data by attention model is analyzed.
